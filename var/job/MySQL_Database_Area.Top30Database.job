/**
 * データベースの中で使用率が高いものを監視します。<br/>
 * <p>
 * 使用中データサイズとインデックスサイズを足したサイズから使用率が高いデータベースを割り出しています<br/>
 * 現在の方法ではファイルの最大サイズを正確に取得できていません。
 * 更新頻度の高いデータベースを監視することで、データサイズの使用率を確認する事ができます。
 * 30個のデータベースのうち20個が使用率が高いもので、残りの10個が前回監視からのデータサイズ使用率の増加が高いものです。
 * </p>
 * @targetVersion 5.1+
 * @return get サンプリング毎に使用率が高い30個のデータベースのレコードを取得します。
 * @returnParam DatabaseNm データベース名
 * @returnParam UsedDataSizeLong 使用中データサイズ(Byte)
 * @returnParam UsedIndexSizeLong 使用中インデックスサイズ(Byte)
 * @returnParam DataFreeSizeLong 空きデータサイズ(Byte)
 * @returnParam UsedSizeLong 使用中データサイズとインデックスサイズを足したサイズ(Byte)
 * @returnParam UsedDataPct データサイズの使用率
 * @returnParam TotalSizeLong 全体のデータサイズ(Byte)
 * @returnParam UsedPct 全体のデータサイズでの使用率
 * @returnParam UsedDiffVal 前回監視からのデータサイズ使用率
 */
JOB = [ name: "MySQL_Database_Area.Top30Database" ]
GROUPKEY = ["Database"]
FETCHACTION={  
	// mapResult is an array, store key and the value of some variable
	def mapResult = [:]
	
	// Store value of datadir variable
	def diskPath
	
	// Free size of the partition which keep the data file
	def diskFreeSize
	
	// Store the status of innodb_file_per_table variable 
	def innodbFilePerTable
	
	// Get datadir to sql
	def diskSize = []
	
	// Get data free size of disk
	def dataFreeSize
	
	// Path to disk create
	def pathCreate
	
	// Path of file create
	def pathFile
	
	// Get value of innodb_file_per_table
	def innodbPerTable
	
	// Get value of innodb_data_file_path
	def innodbFilePath
	
	// Get data
	def resultData = []
	
	// Value of free size of innodb tablespace
	def innodb_tablespace_free_size = 0
	
	// Get data to SQL and add resultData
	def tempResultData = []
	
	//step 1:get data from information_schema.tables
	def data = sql.rows('''
		SELECT table_name, table_schema,data_length ,index_length, max_data_length, engine
		FROM information_schema.tables
		WHERE engine IS NOT NULL AND table_schema NOT IN ('information_schema','mysql','performance_schema')
	''')
	
	//step2 : get free size of disk
	diskSize = sql.rows('''
		select variable_value from information_schema.global_variables where variable_name = 'datadir'
	''')
	diskSize.each{
		mapResult['datadir'] = it['variable_value']
	}
	diskPath = mapResult['datadir']
	
	dataFreeSize = new File(diskPath)
	diskFreeSize = dataFreeSize.getUsableSpace()
	
	//-- assert free size = 0, cause by permission
	assert diskFreeSize > 0, "Disk free size = 0, may cause by user permission while accessing data folder"
		
	innodbPerTable = sql.rows('''
			select variable_value from information_schema.global_variables where variable_name = 'innodb_file_per_table';	
	''')

	innodbPerTable.each{
		mapResult['innodb_file_per_table']=it['variable_value']
	}
	innodbFilePerTable = mapResult['innodb_file_per_table']
		
	def innodbFilePathList = sql.rows('''
			select variable_value from information_schema.global_variables where variable_name = 'innodb_data_file_path'	
	''')
	innodbFilePathList.each{rec->
		if(rec != null && rec != []){
			innodbFilePath = rec['variable_value']
		}
	}
	//-----------------------------------------------------------------------------------------------	
	//get tablespace max size of innodb tablespace
	
	def dataFileList = innodbFilePath == null ?[]:innodbFilePath.split(';')
	// Define max table space size

	Long maxTbsSize = 0
	Long dataFileSize = 0
	dataFileList.each {
	   def dataFileElement = it.split(':')
	   if(it.contains('autoextend')) {
		   if(it.contains('max')) {
			   if(dataFileElement[4].contains('K') && dataFileElement[1].contains('K')) {
				   dataFileSize = dataFileElement[4].replace('K', '').toInteger()*1024
	   			   } else if(dataFileElement[4].contains('M') && dataFileElement[1].contains('M')){
			       dataFileSize = dataFileElement[4].replace('M', '').toInteger()*1024*1024
			   } else {
			       dataFileSize = dataFileElement[4].replace('G', '').toInteger()*1024*1024*1024
			   }
			   maxTbsSize += dataFileSize
		   } else {
		   	   maxTbsSize += diskFreeSize
		   }
	   } else {
		   if(dataFileElement[1].contains('K')) {
			   dataFileSize = dataFileElement[1].replace('K', '').toBigInteger()*1024
		   } else if(dataFileElement[1].contains('M')){
			   dataFileSize = dataFileElement[1].replace('M', '').toBigInteger()*1024*1024
		   } else {
			   dataFileSize = dataFileElement[1].replace('G', '').toBigInteger()*1024*1024*1024
		   }
	       maxTbsSize += dataFileSize
	   }
	}
 	//-----------------------------------------------------------------------------------------------
	//get used tablespace of innodb tablespace
	Long usedTBS = 0
	data.each{		
		if(it['engine'] != null && it['engine'].equalsIgnoreCase('innodb')) {
			usedTBS += it['data_length'] + it['index_length']
		}
	}
	//get innodb_tablespace_free_size if innodb_file_per_table = off
	if(innodbFilePerTable !=null && innodbFilePerTable.equalsIgnoreCase("OFF")){
		if (innodbFilePath!= null && innodbFilePath.contains('autoextend'))
		{
			if(innodbFilePath.contains('max')){
				innodb_tablespace_free_size = Math.min((maxTbsSize - usedTBS),diskFreeSize)
			}else{
				innodb_tablespace_free_size = diskFreeSize
			}	
		}else
		{
			innodb_tablespace_free_size = Math.min((maxTbsSize - usedTBS),diskFreeSize)
		}			
	}
	//-----------------------------------------------------------------------------------------------
	//get table_free_size for each table
	 def mapResultData1 = [:]
 	 
	 data.each{		
	 	mapResultData1['table'] = it['table_name']
		mapResultData1['db'] = it['table_schema']
		mapResultData1['data_length'] = it['data_length']
		mapResultData1['index_length'] = it['index_length']
		mapResultData1['max_data_length'] = it['max_data_length']
		mapResultData1['engine'] = it['engine']
		//-- calculate freesize for each table
		if(it['engine'] != null && it['engine'].equalsIgnoreCase("innodb")){
			if(innodbFilePerTable != null && innodbFilePerTable.equalsIgnoreCase("ON")){
				mapResultData1['free_size_table'] = diskFreeSize
			} else {
				mapResultData1['free_size_table'] = 0
			}
			
		}else if(it['engine'] != null && it['engine'].equalsIgnoreCase("myisam")){
			if((it['max_data_length'] - it['data_length'] - it['index_length']) <= diskFreeSize ){
					mapResultData1['free_size_table'] = it['max_data_length']  - it['data_length'] - it['index_length']		
			}else{
					mapResultData1['free_size_table'] = diskFreeSize
			}
		}else if(it['engine'] != null && it['engine'].equalsIgnoreCase("archive")){
			mapResultData1['free_size_table'] = diskFreeSize
			}else if(it['engine'] != null && it['engine'].equalsIgnoreCase("csv")){
			def pathFileCSV = diskPath + "\\" + it['table_schema'] + "\\" +it['table_name']+".csv"
			File path_FileCSV = new File(pathFileCSV)
			fileCSVSize = path_FileCSV.size()
			mapResultData1['data_length'] = fileCSVSize
			mapResultData1['free_size_table'] = diskFreeSize
		}else{
			mapResultData1['free_size_table'] = 0
		}
		tempResultData.add(mapResultData1)
		mapResultData1 = [:]
	 }

	//-----------------------------------------------------------------------------------------------
	//get final data
	def isExisted
	def innodbFlag = new Hashtable<String, String>()
	tempResultData.each {
		def checkDB = it
		isExisted = false
		resultData.each {		
			if (it['DatabaseNm'].equals(checkDB['db'])) {
				it['UsedDataSizeLong'] += checkDB['data_length']
				it['UsedIndexSizeLong'] += checkDB['index_length']
				it['DataFreeSizeLong'] += checkDB['free_size_table']
				if(checkDB['engine'].equalsIgnoreCase('Innodb')){
					innodbFlag.remove(checkDB['db'])
					innodbFlag.put(checkDB['db'], "true")					
				}
				isExisted = true
			
			}
		}
		if(resultData == null || resultData.size() == 0 || !isExisted) {
			def tempResult = [:]
			tempResult['DatabaseNm'] = checkDB['db']
			tempResult['UsedDataSizeLong'] = checkDB['data_length']
			tempResult['UsedIndexSizeLong'] = checkDB['index_length']
			tempResult['DataFreeSizeLong'] = checkDB['free_size_table']
			if(checkDB['engine'] != null && checkDB['engine'].equalsIgnoreCase('Innodb')){
				innodbFlag.remove(checkDB['db'])
				innodbFlag.put(checkDB['db'], "true")					
			}		
			resultData.add(tempResult)
		}
	}
	
	resultData.each{
		it['UsedSizeLong'] = it['UsedDataSizeLong'] + it['UsedIndexSizeLong']
			
		
		if (it['UsedSizeLong'] == 0) {
			it['UsedDataPct'] = null	
		} else {
			it['UsedDataPct'] = (it['UsedDataSizeLong'] / it['UsedSizeLong'])*100
		}
		if(innodbFlag.containsKey(it['DatabaseNm'])){
			it['DataFreeSizeLong'] += innodb_tablespace_free_size				
		}
		if (it['DataFreeSizeLong'] > diskFreeSize) {
			it['DataFreeSizeLong'] = diskFreeSize
		}
		it['TotalSizeLong'] = it['UsedSizeLong'] + it['DataFreeSizeLong']
			
		it['UsedPct'] = (it['UsedSizeLong'] / it['TotalSizeLong'])*100
	}
	return resultData
}
ACCUMULATE={
	def prevExecutionData = PERSISTENTDATA['prevExecutionData']
	PERSISTENTDATA['prevExecutionData']=[:]
	assert interval != null, "This is the first execution time, interval is null, quit the job"
	//-- Fix bug: prevOUTPUT contain only 30 database so it may cause db not found in previous OUTPUT
	if(PERSISTENTDATA != null && prevExecutionData != null ) {
		OUTPUT.each { rec ->
			rec['UsedDiffVal'] = rec['UsedPct'] - (prevExecutionData[rec.DatabaseNm] != null ? prevExecutionData[rec.DatabaseNm] : 0)
		}
	} else {
	     OUTPUT.each { rec ->
		 	rec['UsedDiffVal'] = 0
		}
	}	
	
	def newPrevExecutionData = [:]
 	OUTPUT.each { rec ->
 		newPrevExecutionData[rec.DatabaseNm] = rec['UsedPct'] 
 	}
 	PERSISTENTDATA['prevExecutionData'] = newPrevExecutionData 
 		
	def resultData = OUTPUT
	def finalResult = [:]
	//get final data
	def arrayMap = resultData.toArray(resultData)
	Arrays.sort(arrayMap, new Comparator() {
		public int compare(arg0, arg1) {
			def p = arg1
			def n = arg0
			float lv = p['UsedPct'] - n['UsedPct']
			if (lv > 0) {
				return 1
			} else if (lv < 0) {
				return -1
			} else {
				return 0
			}
		}	
	})
	// get 20 records
	if (arrayMap.size() > 20) {
 		finalResult['M'] = arrayMap.getAt(0..19)
 	} else {
 		finalResult['M'] = arrayMap
 	}
 	//-----------------------------------------------------------------------------------------------
 	//get 10 DatabaseNm that used data increment fastest
	def arrayMap1 = resultData.toArray(resultData)
	Arrays.sort(arrayMap1, new Comparator() {
		public int compare(arg0, arg1) {
			def p = arg1
			def n = arg0
			float lv = p['UsedDiffVal'] - n['UsedDiffVal']
			if (lv > 0) {
				return 1
			} else if (lv < 0) {
				return -1
			} else {
				return 0
			}
			return lv
		}	
	})
	// get 10 records
	if (arrayMap1.size() > 10) {
 		finalResult['D'] = arrayMap1.getAt(0..9)
 	} else {
 		finalResult['D'] = arrayMap1
 	}
 	OUTPUT = finalResult
}
KEYEXPR = ["M":["DatabaseNm"], "D":["DatabaseNm"], "_sequence": [  "M":["UsedPct", "TotalSizeLong", "UsedDataSizeLong"],"D": ["UsedPct", "TotalSizeLong", "UsedDataSizeLong"] ]]
KEYEXPR._unit = ["UsedDataSizeLong" : "byte" , "UsedSizeLong":"byte", "UsedDataPct":"%","UsedDiffVal" : "byte", "UsedIndexSizeLong" : "byte" ,"DataFreeSizeLong" : "byte", "UsedPct" : "%", "TotalSizeLong" : "byte" ]
DBTYPE = "MYSQL"
DBUSER = "root"
SENDTYPE = "Subtyped"
MONITORINGTYPE = "@DB"
RESOURCEID = "As/MyTopNDb"
DBCONNSTR = parameters.dbconnstr.MYSQL
DEST = parameters.dest
